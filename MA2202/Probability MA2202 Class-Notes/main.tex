\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{prob}
\author{Shuvam Banerji Seal}
\date{January 2024}

\begin{document}

\maketitle

\section{Introduction}
\subsection{Axioms of Probability}
Sample Space ($\omega$): The set of all outcomes of a exp.
Space of events and the ax of prob:
The space of events is a certain set of the powerset of the ($\omega$)

\begin{enumerate}
    \item $\omega \in \epsilon$
    \item If $A \in \epsilon $ then $A_c \in \epsilon$
    \item If $a_i=1 \to \infty$ are events the $U_i=1 to \infty$ A $\in \epsilon$
    \item $\sigma P : \epsilon to$ [0,1 ] st $p(\omega)=1$
    and if $a_i=1 \to \infty$ are pairwise disjoint, (A_i \inU A_j = \phi if i!=j) then
    P(U_i=1 to \infty) = \sum _i=1 \infty P(A_i)

\end{enumerate}

\subsubsection{Example}
1. Tossing a coin $\omega$ = {H,T}
$\epsilon$ = {$\phi$, {H}, {T}, {H,T}}
P($\phi$) = 0
P(H) = p
P(T) = 1-p
P($\omega$) = 1

\subsubsection{Ex2}
2. Rolling a die
$\omega$ = {1,2,3,4,5,6}

\subsubsection{Example 3:}
Guessing the first letter of the person's name 
$\omega$ = {A,B, \dots, z}

\subsubsection{Example 4:}
Guessing the number of the stars in the milky way
$\omega$ = {1,2,3, \dots }

\subsubsection{Example 5:}

Guessing someone's palm temperature( in $\degree $F)
$\omega$ = [55,115]

\subsubsection{Example: 6 (Anubhav)}
A= Someone has fever
A= [c,115 ] for some c
$A_c$ [55,c]

$\epsilon$ = {$\phi$, A, A_c, $\omega$}
\section{NOTE:}
Vita-Li set where probability can't be defined due to sample space being \infty.
%Reminder to color it later using tikz

\section{Definitions:}
\begin{enumerate}
    \item \textbf{Mutually Exclusive Events:} A,B \in \epsilon are called MEE if A \intersect B = \phi
    then, P(A U B) = P(A) + P(B)

    \item When A \intersect B != \phi 
    A= (A \intersect B_c) U (A \intersect B)
    P(A) = P(A \intersect B_c) + P(A \intersect B)
    A U B = B U (A \intersect B_c)
    P(A U B) = P(B) + P(A \intersect B_c)
    P(A \intersect B_c) = P(A U B) - P(B)
    P (A) = P(A U B) - P(B) + P(A \intersect B)
    P(A U B) = P(A) + P(B) - P (A \intersect B)\\

    \textbf{Notes:} By induction, this notion can be generalized for arbitary n \in N.
    \textbf{Notes_2:} 
    \begin{equation}
        |A U B| = |A| + |B| - |A \intersect B|
        | A_1 U A_2 U \dots U A_n| = \sum_{i=1}^{n}|A_i| - \sum_{i,j}^{n}|A_i \intersect B_j|
    \end{equation}
    \begin{align}
P\Big( \bigcup_{i=1}^{n}A_i\Big)
=&-(-1)^1\sum_{i{_1}=1}^{n}P(A_{i_1})
\\
&-(-1)^2\sum_{1\leq i{_1}<i_{_2}\leq n}^{n}P(A_{i_1}\cap A_{i_2})
\\
&-(-1)^3\sum_{1\leq i{_1}<i_{_2}<i_{_3}\leq n}^{n}P(A_{i_1}\cap A_{i_2}\cap A_{i_3})
\\
&-(-1)^4\sum_{1\leq i{_1}<i_{_2}<i_{_3}<i_{_4}\leq n}^{n}P(A_{i_1}\cap A_{i_2}\cap A_{i_3}\cap A_{i_4})
\\
&\quad\quad\quad\vdots
\\
&\ldots-(-1)^n \sum_{1\leq i_{1}<i_{2}<i_{3}<\ldots <i_{n}\leq n}^{n}P(A_{i_1}\cap A_{i_2}\cap A_{i_3}\cap \ldots \cap A_{i_n})
\end{align}

\dots Principle of Exclusion and Inclusion

    \item \textbf{Exhaustive Set of events:} A set S $\subset$ \epsilon is an exhaustive set of events (or the events in S to be exhaustive) if $\bigcup\limits_{A \in S}^ A = \omega$.

    Let $\omega = {W_n}_{n=1}^{\infty} $ be a countable sample space st \epsilon = 2^{$\omega$} 
    {W_n} \in \epsilon \forall W_n \in \omega

    Now, we have
    1 = P(\omega) = P($\bigcup\limits_{n=1}^{\infty} W_{n}$) = ($\sum\limits_{n=1}^{\infty} P(W_{n})$)
    so, \sum\limits_{n=1}^{\infty} P(W_n)= 1

    \textbf{Example:}
    Let p>0 be the probability of obtaining head if a coin is tossed. Show that if we keep on tossing the coin, then the probability of obtaining a head is eventually 1.
    \textbf{Soln:}
    \omega = {H, TH,, TTH, TTTH, \dots} \cup {T, TT, TTT, \dots}
    P(H) = p
    P(TH) = (1-p)p
    P(TTH) = (1-p)^2p
    \dots
    P({H, TH,, TTH, TTTH, \dots}) = P(H) + P(TH) + \dots
    = p + (1-p)p + (1-p)^2p + \dots
    = p \sum\limits_{n=0}^{\infty} (1-p)^n
    = \frac{p}{1-(1-p)} = 1
        




\subsubsection{Example:}
A \subset \epsilon
P(A) = \sum\limits_{W_n \in A}^{} P(W_n)
\subsection{Roll a die:}
\omega = {1,2, \dots , 6}
A = {1,3,5} ie odd number
if you roll a die and if the roll results in an odd number we say that the event A has occurred 
then B = { 2,3,5}

\textbf{Note:} if the outcome is 3 then we say that both A and B has happened.
A \in \epsilon
P(A) = \sum_{W_n \in A} P(W_n)


        \item \textbf{Equally likely events:} Let S \subset \epsilon we say the events in S are equally likely if P(A) = P(B)
        for all A,B \in S
        More often, we say the events A and B are equally likely if P(A) = P(B) \dots Classical definition of Probability.
        \item \textbf{Random Experiment:} An experiment of which we know the sample space but none of the outcomes occurs with certainty.
        \item \textbf{Non-Random Experiment:} Experiments for verificationn of physical, chemical, biological or mathematical laws.
        



        
\end{enumerate}

\section{Motivation from Number Theory:}
if you have a set of integers [1, \dots, N]
\phi (N) = The number of integers co-prime to N in {1, \dots, N}
K^{th} Primodial: Probability of first co-prime
What is the probability of obtaining a number from {1, \dots, N} which is co-prime  with N_{k} =  \frac{\phi (N_k)}{N_k} < \frac{1}{e^\gamma log log N_k}
Proving this is equivalent to proving Reimann Hypothesis
\end{document}
